{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "501ec0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.57.1'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a095f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiatronForCausalLM(\n",
       "  (model): BiatronModel(\n",
       "    (embed_tokens): Embedding(32000, 960, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x BiatronDecoderLayer(\n",
       "        (self_attn): BiatronAttention(\n",
       "          (q_proj): Linear(in_features=960, out_features=960, bias=True)\n",
       "          (k_proj): Linear(in_features=960, out_features=320, bias=True)\n",
       "          (v_proj): Linear(in_features=960, out_features=320, bias=True)\n",
       "          (o_proj): Linear(in_features=960, out_features=960, bias=True)\n",
       "        )\n",
       "        (mlp): BiatronMLP(\n",
       "          (up_proj): Linear(in_features=960, out_features=3840, bias=True)\n",
       "          (down_proj): Linear(in_features=3840, out_features=960, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): BiatronRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=960, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Biatron.config import BiatronConfig\n",
    "from Biatron.modeling_biatron import BiatronForCausalLM\n",
    "\n",
    "\n",
    "model = BiatronForCausalLM(BiatronConfig())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a852b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_convert import load_state_dict_biatron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24b63554",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_state_dict_biatron(model, \"/home/dan/CEIA/MEGATRON/convert/dict_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ca55b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiatronForCausalLM(\n",
       "  (model): BiatronModel(\n",
       "    (embed_tokens): Embedding(32000, 960, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x BiatronDecoderLayer(\n",
       "        (self_attn): BiatronAttention(\n",
       "          (q_proj): Linear(in_features=960, out_features=960, bias=True)\n",
       "          (k_proj): Linear(in_features=960, out_features=320, bias=True)\n",
       "          (v_proj): Linear(in_features=960, out_features=320, bias=True)\n",
       "          (o_proj): Linear(in_features=960, out_features=960, bias=True)\n",
       "        )\n",
       "        (mlp): BiatronMLP(\n",
       "          (up_proj): Linear(in_features=960, out_features=3840, bias=True)\n",
       "          (down_proj): Linear(in_features=3840, out_features=960, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): BiatronRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=960, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "072967fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -6.8392,   8.6455,  -7.1738,  ...,  -1.4409,  -1.0301,  -1.7648],\n",
       "         [ -9.0968,   4.3056,  -9.1296,  ...,  -1.4099,  -1.9358,  -2.9704],\n",
       "         [ -8.7057,   6.1272,  -9.4507,  ...,  -1.4659,  -2.0700,  -4.0830],\n",
       "         ...,\n",
       "         [-10.8051,   7.4513, -11.5286,  ...,  -6.0487,  -3.2882,  -3.9321],\n",
       "         [ -9.4184,   6.6918, -10.2852,  ...,  -5.2178,  -2.8604,  -4.1874],\n",
       "         [-10.2871,   8.5075, -11.0755,  ...,  -5.4560,  -2.9234,  -5.3958]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], device='cpu')\n",
    "model.config._attn_implementation = 'sdpa'\n",
    "model.eval()\n",
    "output = model(input_ids=input_ids)\n",
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62a98be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Fazzioni/tokenizer-fineweb.50M-32k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "902e718a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  355, 1276,  298,  299, 2907,  179]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= ' O brasil é um pais '\n",
    "input_ids = tokenizer(text, return_tensors='pt').input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ead6b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_new_tokens=200, do_sample=True, top_p=0.9, temperature=0.8, top_k=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c63cd945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' O brasil é um pais de enorme desigualdade social, a desigualdade social esta presente em todas as esferas da sociedade, isso e claro, se reflete no sistema de educacao. Mas uma educacao de qualidade e a base para uma sociedade justa. A educacao de qualidade e fundamental para a sociedade brasileira, pois a educacao de qualidade nao garante a todos o acesso a educacao, ela garante o acesso as universidades e cursos profissionalizantes, cursos que o governo e as empresas privadas oferecem, e o acesso a um ensino de qualidade. A educacao de qualidade nao pode ser a unica solucao para a crise, pois ela tem que ser parte do processo de desenvolvimento, por isso e preciso investir no sistema de educacao, e necessario investir em recursos humanos, materiais e tecnologicos para melhorar o ensino. A educacao de qualidade e necessaria para todos os brasileiros, nao adianta somente a escola de qualidade,'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8d720139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345653120"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number os parameters\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8de13f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
