{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba2c4835",
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install lighteval\n",
    "#!pip install \"lighteval[all]\"\n",
    "#!pip install langdetect\n",
    "#!pip install lighteval[multilingual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b75a49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from biatron import BiatronForCausalLM, BiatronConfig\n",
    "#AutoConfig.register(\"Biatron\", BiatronConfig)\n",
    "#AutoModelForCausalLM.register(BiatronConfig, BiatronForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e642ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "bench  = {}\n",
    "\n",
    "#biamodel = BiatronForCausalLM.from_pretrained(\"\", torch_dtype=torch.bfloat16, device_map=\"cuda\", use_cache=False, _attn_implementation='sdpa', revision=\"checkpoint-152000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
    "from lighteval.models.transformers.transformers_model import TransformersModel, TransformersModelConfig\n",
    "from lighteval.pipeline import ParallelismManager, Pipeline, PipelineParameters\n",
    "\n",
    "BENCHMARKS = \"enem_por_mcf,oab_exams_por_mcf,exams_por_mcf,m3exams_por_mcf,openai_mmlu_por_mcf\"\n",
    "\n",
    "for model_name in ['Fazzioni/biatron-345m',\n",
    "                   'google/gemma-3-270m',\n",
    "                   'google/gemma-3-1B-pt',\n",
    "                   'TucanoBR/Tucano-630m',\n",
    "                   'HuggingFaceTB/SmolLM2-360M',\n",
    "                   'Qwen/Qwen3-0.6B-Base'\n",
    "                   'TucanoBR/Tucano-160m'\n",
    "                   ]:\n",
    "    print(\"STARTING EVAL FOR MODEL:\", model_name)\n",
    "    \n",
    "    evaluation_tracker = EvaluationTracker(output_dir=\"./results\")\n",
    "    pipeline_params = PipelineParameters(\n",
    "        launcher_type=ParallelismManager.NONE,\n",
    "        load_tasks_multilingual=True\n",
    "    )\n",
    "    \n",
    "    CLASS_NAME = AutoModelForCausalLM if 'biatron' not in model_name else BiatronForCausalLM\n",
    "    kwargs = {}\n",
    "    if 'biatron' in model_name:\n",
    "        kwargs = {'revision':'checkpoint-152000'}\n",
    "        \n",
    "    model = CLASS_NAME.from_pretrained(model_name, device_map=\"cuda\", dtype=torch.bfloat16, **kwargs)\n",
    "    \n",
    "    config = TransformersModelConfig(model_name=model.config._name_or_path, batch_size=1)\n",
    "    Transmodel = TransformersModel.from_model(model, config)\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        model=Transmodel,\n",
    "        pipeline_parameters=pipeline_params,\n",
    "        evaluation_tracker=evaluation_tracker,\n",
    "        tasks=BENCHMARKS,\n",
    "    )\n",
    "    \n",
    "    results = pipeline.evaluate()\n",
    "    pipeline.show_results()\n",
    "    results = pipeline.get_results()\n",
    "\n",
    "    #m = model.config._name_or_path\n",
    "    bench[model_name] = {}\n",
    "    for k,v in results['results'].items():\n",
    "        bench[model_name][k] = v['acc']\n",
    "\n",
    "    del model\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4378476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Fazzioni/biatron-345m', 'google/gemma-3-270m', 'google/gemma-3-1B-pt', 'TucanoBR/Tucano-630m', 'HuggingFaceTB/SmolLM2-360M', 'Qwen/Qwen3-0.6B-Base', 'TucanoBR/Tucano-160m'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17978972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                                           |   Fazzioni/biatron-345m |   google/gemma-3-270m |   google/gemma-3-1B-pt |   TucanoBR/Tucano-630m |   HuggingFaceTB/SmolLM2-360M |   Qwen/Qwen3-0.6B-Base |   TucanoBR/Tucano-160m |\n",
      "|:----------------------------------------------------------|------------------------:|----------------------:|-----------------------:|-----------------------:|-----------------------------:|-----------------------:|-----------------------:|\n",
      "| m3exams_por_mcf:0                                         |                0.225    |              0.202273 |               0.196591 |               0.201136 |                     0.197727 |               0.197727 |               0.190909 |\n",
      "| enem_por_mcf:2022:0                                       |                0.212291 |              0.195531 |               0.206704 |               0.178771 |                     0.195531 |               0.206704 |               0.212291 |\n",
      "| enem_por_mcf:2023:0                                       |                0.24581  |              0.24581  |               0.173184 |               0.206704 |                     0.234637 |               0.240223 |               0.240223 |\n",
      "| enem_por_mcf:2024:0                                       |                0.189944 |              0.167598 |               0.217877 |               0.206704 |                     0.173184 |               0.173184 |               0.173184 |\n",
      "| openai_mmlu_por_mcf:abstract_algebra:0                    |                0.31     |              0.22     |               0.31     |               0.2      |                     0.24     |               0.22     |               0.22     |\n",
      "| openai_mmlu_por_mcf:anatomy:0                             |                0.237037 |              0.185185 |               0.288889 |               0.244444 |                     0.222222 |               0.185185 |               0.237037 |\n",
      "| openai_mmlu_por_mcf:astronomy:0                           |                0.223684 |              0.190789 |               0.328947 |               0.230263 |                     0.197368 |               0.177632 |               0.184211 |\n",
      "| openai_mmlu_por_mcf:business_ethics:0                     |                0.23     |              0.3      |               0.19     |               0.25     |                     0.22     |               0.3      |               0.31     |\n",
      "| openai_mmlu_por_mcf:clinical_knowledge:0                  |                0.245283 |              0.211321 |               0.241509 |               0.264151 |                     0.203774 |               0.215094 |               0.222642 |\n",
      "| openai_mmlu_por_mcf:college_biology:0                     |                0.263889 |              0.243056 |               0.243056 |               0.243056 |                     0.236111 |               0.256944 |               0.270833 |\n",
      "| openai_mmlu_por_mcf:college_chemistry:0                   |                0.28     |              0.21     |               0.36     |               0.33     |                     0.22     |               0.2      |               0.18     |\n",
      "| openai_mmlu_por_mcf:college_computer_science:0            |                0.32     |              0.27     |               0.32     |               0.28     |                     0.25     |               0.26     |               0.25     |\n",
      "| openai_mmlu_por_mcf:college_mathematics:0                 |                0.24     |              0.21     |               0.27     |               0.26     |                     0.25     |               0.21     |               0.18     |\n",
      "| openai_mmlu_por_mcf:college_medicine:0                    |                0.231214 |              0.202312 |               0.312139 |               0.242775 |                     0.196532 |               0.208092 |               0.213873 |\n",
      "| openai_mmlu_por_mcf:college_physics:0                     |                0.22549  |              0.215686 |               0.362745 |               0.313725 |                     0.205882 |               0.215686 |               0.22549  |\n",
      "| openai_mmlu_por_mcf:computer_security:0                   |                0.27     |              0.28     |               0.22     |               0.21     |                     0.28     |               0.28     |               0.27     |\n",
      "| openai_mmlu_por_mcf:conceptual_physics:0                  |                0.26383  |              0.27234  |               0.187234 |               0.178723 |                     0.246809 |               0.26383  |               0.280851 |\n",
      "| openai_mmlu_por_mcf:econometrics:0                        |                0.192982 |              0.22807  |               0.219298 |               0.184211 |                     0.219298 |               0.236842 |               0.245614 |\n",
      "| openai_mmlu_por_mcf:electrical_engineering:0              |                0.262069 |              0.227586 |               0.275862 |               0.186207 |                     0.262069 |               0.241379 |               0.255172 |\n",
      "| openai_mmlu_por_mcf:elementary_mathematics:0              |                0.232804 |              0.198413 |               0.238095 |               0.251323 |                     0.224868 |               0.208995 |               0.227513 |\n",
      "| openai_mmlu_por_mcf:formal_logic:0                        |                0.222222 |              0.269841 |               0.301587 |               0.253968 |                     0.293651 |               0.285714 |               0.253968 |\n",
      "| openai_mmlu_por_mcf:global_facts:0                        |                0.26     |              0.19     |               0.34     |               0.21     |                     0.24     |               0.18     |               0.18     |\n",
      "| openai_mmlu_por_mcf:high_school_biology:0                 |                0.245161 |              0.187097 |               0.251613 |               0.3      |                     0.187097 |               0.177419 |               0.174194 |\n",
      "| openai_mmlu_por_mcf:high_school_chemistry:0               |                0.20197  |              0.152709 |               0.285714 |               0.256158 |                     0.17734  |               0.152709 |               0.211823 |\n",
      "| openai_mmlu_por_mcf:high_school_computer_science:0        |                0.35     |              0.25     |               0.3      |               0.22     |                     0.23     |               0.25     |               0.25     |\n",
      "| openai_mmlu_por_mcf:high_school_european_history:0        |                0.212121 |              0.218182 |               0.206061 |               0.230303 |                     0.230303 |               0.218182 |               0.218182 |\n",
      "| openai_mmlu_por_mcf:high_school_geography:0               |                0.348485 |              0.181818 |               0.272727 |               0.30303  |                     0.20202  |               0.176768 |               0.186869 |\n",
      "| openai_mmlu_por_mcf:high_school_government_and_politics:0 |                0.274611 |              0.196891 |               0.300518 |               0.305699 |                     0.227979 |               0.196891 |               0.196891 |\n",
      "| openai_mmlu_por_mcf:high_school_macroeconomics:0          |                0.279487 |              0.210256 |               0.225641 |               0.287179 |                     0.230769 |               0.202564 |               0.205128 |\n",
      "| openai_mmlu_por_mcf:high_school_mathematics:0             |                0.248148 |              0.225926 |               0.281481 |               0.281481 |                     0.233333 |               0.211111 |               0.222222 |\n",
      "| openai_mmlu_por_mcf:high_school_microeconomics:0          |                0.252101 |              0.205882 |               0.247899 |               0.323529 |                     0.222689 |               0.210084 |               0.205882 |\n",
      "| openai_mmlu_por_mcf:high_school_physics:0                 |                0.218543 |              0.205298 |               0.258278 |               0.278146 |                     0.198675 |               0.198675 |               0.218543 |\n",
      "| openai_mmlu_por_mcf:high_school_psychology:0              |                0.245872 |              0.19633  |               0.26055  |               0.238532 |                     0.201835 |               0.192661 |               0.188991 |\n",
      "| openai_mmlu_por_mcf:high_school_statistics:0              |                0.199074 |              0.180556 |               0.268519 |               0.425926 |                     0.157407 |               0.152778 |               0.143519 |\n",
      "| openai_mmlu_por_mcf:high_school_us_history:0              |                0.27451  |              0.240196 |               0.245098 |               0.264706 |                     0.25     |               0.25     |               0.25     |\n",
      "| openai_mmlu_por_mcf:high_school_world_history:0           |                0.223629 |              0.265823 |               0.291139 |               0.244726 |                     0.265823 |               0.270042 |               0.261603 |\n",
      "| openai_mmlu_por_mcf:human_aging:0                         |                0.255605 |              0.29148  |               0.26009  |               0.313901 |                     0.286996 |               0.313901 |               0.318386 |\n",
      "| openai_mmlu_por_mcf:human_sexuality:0                     |                0.290076 |              0.236641 |               0.206107 |               0.198473 |                     0.259542 |               0.259542 |               0.259542 |\n",
      "| openai_mmlu_por_mcf:international_law:0                   |                0.198347 |              0.239669 |               0.140496 |               0.214876 |                     0.206612 |               0.239669 |               0.239669 |\n",
      "| openai_mmlu_por_mcf:jurisprudence:0                       |                0.231481 |              0.25     |               0.175926 |               0.259259 |                     0.268519 |               0.259259 |               0.259259 |\n",
      "| openai_mmlu_por_mcf:logical_fallacies:0                   |                0.233129 |              0.214724 |               0.294479 |               0.233129 |                     0.214724 |               0.220859 |               0.220859 |\n",
      "| openai_mmlu_por_mcf:machine_learning:0                    |                0.258929 |              0.276786 |               0.214286 |               0.205357 |                     0.330357 |               0.3125   |               0.330357 |\n",
      "| openai_mmlu_por_mcf:management:0                          |                0.23301  |              0.184466 |               0.252427 |               0.203883 |                     0.203883 |               0.174757 |               0.174757 |\n",
      "| openai_mmlu_por_mcf:marketing:0                           |                0.196581 |              0.286325 |               0.247863 |               0.24359  |                     0.269231 |               0.290598 |               0.260684 |\n",
      "| openai_mmlu_por_mcf:medical_genetics:0                    |                0.23     |              0.33     |               0.32     |               0.29     |                     0.35     |               0.3      |               0.28     |\n",
      "| openai_mmlu_por_mcf:miscellaneous:0                       |                0.257982 |              0.240102 |               0.268199 |               0.208174 |                     0.261814 |               0.237548 |               0.250319 |\n",
      "| openai_mmlu_por_mcf:moral_disputes:0                      |                0.268786 |              0.254335 |               0.265896 |               0.222543 |                     0.280347 |               0.248555 |               0.245665 |\n",
      "| openai_mmlu_por_mcf:moral_scenarios:0                     |                0.272626 |              0.24581  |               0.246927 |               0.258101 |                     0.246927 |               0.237989 |               0.237989 |\n",
      "| openai_mmlu_por_mcf:nutrition:0                           |                0.248366 |              0.22549  |               0.261438 |               0.24183  |                     0.235294 |               0.22549  |               0.218954 |\n",
      "| openai_mmlu_por_mcf:philosophy:0                          |                0.237942 |              0.189711 |               0.266881 |               0.192926 |                     0.202572 |               0.186495 |               0.196141 |\n",
      "| openai_mmlu_por_mcf:prehistory:0                          |                0.231481 |              0.212963 |               0.234568 |               0.25     |                     0.243827 |               0.216049 |               0.222222 |\n",
      "| openai_mmlu_por_mcf:professional_accounting:0             |                0.237589 |              0.230496 |               0.269504 |               0.283688 |                     0.280142 |               0.234043 |               0.234043 |\n",
      "| openai_mmlu_por_mcf:professional_law:0                    |                0.231421 |              0.246415 |               0.254237 |               0.245763 |                     0.25163  |               0.245763 |               0.247066 |\n",
      "| openai_mmlu_por_mcf:professional_medicine:0               |                0.227941 |              0.1875   |               0.272059 |               0.25     |                     0.198529 |               0.183824 |               0.180147 |\n",
      "| openai_mmlu_por_mcf:professional_psychology:0             |                0.235294 |              0.240196 |               0.245098 |               0.21732  |                     0.254902 |               0.25     |               0.248366 |\n",
      "| openai_mmlu_por_mcf:public_relations:0                    |                0.227273 |              0.218182 |               0.209091 |               0.281818 |                     0.218182 |               0.218182 |               0.227273 |\n",
      "| openai_mmlu_por_mcf:security_studies:0                    |                0.240816 |              0.191837 |               0.24898  |               0.310204 |                     0.195918 |               0.187755 |               0.187755 |\n",
      "| openai_mmlu_por_mcf:sociology:0                           |                0.243781 |              0.263682 |               0.273632 |               0.268657 |                     0.243781 |               0.243781 |               0.243781 |\n",
      "| openai_mmlu_por_mcf:us_foreign_policy:0                   |                0.24     |              0.28     |               0.26     |               0.27     |                     0.33     |               0.28     |               0.28     |\n",
      "| openai_mmlu_por_mcf:virology:0                            |                0.259036 |              0.283133 |               0.222892 |               0.210843 |                     0.259036 |               0.283133 |               0.289157 |\n",
      "| openai_mmlu_por_mcf:world_religions:0                     |                0.280702 |              0.309942 |               0.298246 |               0.315789 |                     0.28655  |               0.321637 |               0.333333 |\n",
      "| exams_por_mcf:biology:0                                   |                0.238636 |              0.227273 |               0.255682 |               0.295455 |                     0.193182 |               0.232955 |               0.232955 |\n",
      "| exams_por_mcf:economics:0                                 |                0.18018  |              0.279279 |               0.252252 |               0.207207 |                     0.261261 |               0.279279 |               0.279279 |\n",
      "| exams_por_mcf:geology:0                                   |                0.275862 |              0.241379 |               0.224138 |               0.232759 |                     0.232759 |               0.241379 |               0.241379 |\n",
      "| exams_por_mcf:philosophy:0                                |                0.2      |              0.133333 |               0.266667 |               0.166667 |                     0.166667 |               0.133333 |               0.133333 |\n",
      "| oab_exams_por_mcf:0                                       |                0.245249 |              0.230317 |               0.242986 |               0.246606 |                     0.230769 |               0.229864 |               0.229412 |\n",
      "| enem_por_mcf:_average:0                                   |                0.216015 |              0.20298  |               0.199255 |               0.197393 |                     0.201117 |               0.206704 |               0.208566 |\n",
      "| openai_mmlu_por_mcf:_average:0                            |                0.248288 |              0.231078 |               0.261648 |               0.254077 |                     0.238652 |               0.231169 |               0.233803 |\n",
      "| exams_por_mcf:_average:0                                  |                0.22367  |              0.220316 |               0.249685 |               0.225522 |                     0.213467 |               0.221737 |               0.221737 |\n",
      "| all                                                       |                0.24493  |              0.228701 |               0.256818 |               0.248854 |                     0.23468  |               0.228958 |               0.231208 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(bench)\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a12d6512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                                           | Fazzioni/biatron-345m   | google/gemma-3-270m   | google/gemma-3-1B-pt   | TucanoBR/Tucano-630m   | HuggingFaceTB/SmolLM2-360M   | Qwen/Qwen3-0.6B-Base   | TucanoBR/Tucano-160m   |\n",
      "|:----------------------------------------------------------|:------------------------|:----------------------|:-----------------------|:-----------------------|:-----------------------------|:-----------------------|:-----------------------|\n",
      "| m3exams_por_mcf:0                                         | **0.225**               | 0.202                 | 0.197                  | 0.201                  | 0.198                        | 0.198                  | 0.191                  |\n",
      "| enem_por_mcf:2022:0                                       | **0.212**               | 0.196                 | 0.207                  | 0.179                  | 0.196                        | 0.207                  | **0.212**              |\n",
      "| enem_por_mcf:2023:0                                       | **0.246**               | **0.246**             | 0.173                  | 0.207                  | 0.235                        | 0.24                   | 0.24                   |\n",
      "| enem_por_mcf:2024:0                                       | 0.19                    | 0.168                 | **0.218**              | 0.207                  | 0.173                        | 0.173                  | 0.173                  |\n",
      "| openai_mmlu_por_mcf:abstract_algebra:0                    | **0.31**                | 0.22                  | **0.31**               | 0.2                    | 0.24                         | 0.22                   | 0.22                   |\n",
      "| openai_mmlu_por_mcf:anatomy:0                             | 0.237                   | 0.185                 | **0.289**              | 0.244                  | 0.222                        | 0.185                  | 0.237                  |\n",
      "| openai_mmlu_por_mcf:astronomy:0                           | 0.224                   | 0.191                 | **0.329**              | 0.23                   | 0.197                        | 0.178                  | 0.184                  |\n",
      "| openai_mmlu_por_mcf:business_ethics:0                     | 0.23                    | 0.3                   | 0.19                   | 0.25                   | 0.22                         | 0.3                    | **0.31**               |\n",
      "| openai_mmlu_por_mcf:clinical_knowledge:0                  | 0.245                   | 0.211                 | 0.242                  | **0.264**              | 0.204                        | 0.215                  | 0.223                  |\n",
      "| openai_mmlu_por_mcf:college_biology:0                     | 0.264                   | 0.243                 | 0.243                  | 0.243                  | 0.236                        | 0.257                  | **0.271**              |\n",
      "| openai_mmlu_por_mcf:college_chemistry:0                   | 0.28                    | 0.21                  | **0.36**               | 0.33                   | 0.22                         | 0.2                    | 0.18                   |\n",
      "| openai_mmlu_por_mcf:college_computer_science:0            | **0.32**                | 0.27                  | **0.32**               | 0.28                   | 0.25                         | 0.26                   | 0.25                   |\n",
      "| openai_mmlu_por_mcf:college_mathematics:0                 | 0.24                    | 0.21                  | **0.27**               | 0.26                   | 0.25                         | 0.21                   | 0.18                   |\n",
      "| openai_mmlu_por_mcf:college_medicine:0                    | 0.231                   | 0.202                 | **0.312**              | 0.243                  | 0.197                        | 0.208                  | 0.214                  |\n",
      "| openai_mmlu_por_mcf:college_physics:0                     | 0.225                   | 0.216                 | **0.363**              | 0.314                  | 0.206                        | 0.216                  | 0.225                  |\n",
      "| openai_mmlu_por_mcf:computer_security:0                   | 0.27                    | **0.28**              | 0.22                   | 0.21                   | **0.28**                     | **0.28**               | 0.27                   |\n",
      "| openai_mmlu_por_mcf:conceptual_physics:0                  | 0.264                   | 0.272                 | 0.187                  | 0.179                  | 0.247                        | 0.264                  | **0.281**              |\n",
      "| openai_mmlu_por_mcf:econometrics:0                        | 0.193                   | 0.228                 | 0.219                  | 0.184                  | 0.219                        | 0.237                  | **0.246**              |\n",
      "| openai_mmlu_por_mcf:electrical_engineering:0              | 0.262                   | 0.228                 | **0.276**              | 0.186                  | 0.262                        | 0.241                  | 0.255                  |\n",
      "| openai_mmlu_por_mcf:elementary_mathematics:0              | 0.233                   | 0.198                 | 0.238                  | **0.251**              | 0.225                        | 0.209                  | 0.228                  |\n",
      "| openai_mmlu_por_mcf:formal_logic:0                        | 0.222                   | 0.27                  | **0.302**              | 0.254                  | 0.294                        | 0.286                  | 0.254                  |\n",
      "| openai_mmlu_por_mcf:global_facts:0                        | 0.26                    | 0.19                  | **0.34**               | 0.21                   | 0.24                         | 0.18                   | 0.18                   |\n",
      "| openai_mmlu_por_mcf:high_school_biology:0                 | 0.245                   | 0.187                 | 0.252                  | **0.3**                | 0.187                        | 0.177                  | 0.174                  |\n",
      "| openai_mmlu_por_mcf:high_school_chemistry:0               | 0.202                   | 0.153                 | **0.286**              | 0.256                  | 0.177                        | 0.153                  | 0.212                  |\n",
      "| openai_mmlu_por_mcf:high_school_computer_science:0        | **0.35**                | 0.25                  | 0.3                    | 0.22                   | 0.23                         | 0.25                   | 0.25                   |\n",
      "| openai_mmlu_por_mcf:high_school_european_history:0        | 0.212                   | 0.218                 | 0.206                  | **0.23**               | **0.23**                     | 0.218                  | 0.218                  |\n",
      "| openai_mmlu_por_mcf:high_school_geography:0               | **0.348**               | 0.182                 | 0.273                  | 0.303                  | 0.202                        | 0.177                  | 0.187                  |\n",
      "| openai_mmlu_por_mcf:high_school_government_and_politics:0 | 0.275                   | 0.197                 | 0.301                  | **0.306**              | 0.228                        | 0.197                  | 0.197                  |\n",
      "| openai_mmlu_por_mcf:high_school_macroeconomics:0          | 0.279                   | 0.21                  | 0.226                  | **0.287**              | 0.231                        | 0.203                  | 0.205                  |\n",
      "| openai_mmlu_por_mcf:high_school_mathematics:0             | 0.248                   | 0.226                 | **0.281**              | **0.281**              | 0.233                        | 0.211                  | 0.222                  |\n",
      "| openai_mmlu_por_mcf:high_school_microeconomics:0          | 0.252                   | 0.206                 | 0.248                  | **0.324**              | 0.223                        | 0.21                   | 0.206                  |\n",
      "| openai_mmlu_por_mcf:high_school_physics:0                 | 0.219                   | 0.205                 | 0.258                  | **0.278**              | 0.199                        | 0.199                  | 0.219                  |\n",
      "| openai_mmlu_por_mcf:high_school_psychology:0              | 0.246                   | 0.196                 | **0.261**              | 0.239                  | 0.202                        | 0.193                  | 0.189                  |\n",
      "| openai_mmlu_por_mcf:high_school_statistics:0              | 0.199                   | 0.181                 | 0.269                  | **0.426**              | 0.157                        | 0.153                  | 0.144                  |\n",
      "| openai_mmlu_por_mcf:high_school_us_history:0              | **0.275**               | 0.24                  | 0.245                  | 0.265                  | 0.25                         | 0.25                   | 0.25                   |\n",
      "| openai_mmlu_por_mcf:high_school_world_history:0           | 0.224                   | 0.266                 | **0.291**              | 0.245                  | 0.266                        | 0.27                   | 0.262                  |\n",
      "| openai_mmlu_por_mcf:human_aging:0                         | 0.256                   | 0.291                 | 0.26                   | 0.314                  | 0.287                        | 0.314                  | **0.318**              |\n",
      "| openai_mmlu_por_mcf:human_sexuality:0                     | **0.29**                | 0.237                 | 0.206                  | 0.198                  | 0.26                         | 0.26                   | 0.26                   |\n",
      "| openai_mmlu_por_mcf:international_law:0                   | 0.198                   | **0.24**              | 0.14                   | 0.215                  | 0.207                        | **0.24**               | **0.24**               |\n",
      "| openai_mmlu_por_mcf:jurisprudence:0                       | 0.231                   | 0.25                  | 0.176                  | 0.259                  | **0.269**                    | 0.259                  | 0.259                  |\n",
      "| openai_mmlu_por_mcf:logical_fallacies:0                   | 0.233                   | 0.215                 | **0.294**              | 0.233                  | 0.215                        | 0.221                  | 0.221                  |\n",
      "| openai_mmlu_por_mcf:machine_learning:0                    | 0.259                   | 0.277                 | 0.214                  | 0.205                  | **0.33**                     | 0.312                  | **0.33**               |\n",
      "| openai_mmlu_por_mcf:management:0                          | 0.233                   | 0.184                 | **0.252**              | 0.204                  | 0.204                        | 0.175                  | 0.175                  |\n",
      "| openai_mmlu_por_mcf:marketing:0                           | 0.197                   | 0.286                 | 0.248                  | 0.244                  | 0.269                        | **0.291**              | 0.261                  |\n",
      "| openai_mmlu_por_mcf:medical_genetics:0                    | 0.23                    | 0.33                  | 0.32                   | 0.29                   | **0.35**                     | 0.3                    | 0.28                   |\n",
      "| openai_mmlu_por_mcf:miscellaneous:0                       | 0.258                   | 0.24                  | **0.268**              | 0.208                  | 0.262                        | 0.238                  | 0.25                   |\n",
      "| openai_mmlu_por_mcf:moral_disputes:0                      | 0.269                   | 0.254                 | 0.266                  | 0.223                  | **0.28**                     | 0.249                  | 0.246                  |\n",
      "| openai_mmlu_por_mcf:moral_scenarios:0                     | **0.273**               | 0.246                 | 0.247                  | 0.258                  | 0.247                        | 0.238                  | 0.238                  |\n",
      "| openai_mmlu_por_mcf:nutrition:0                           | 0.248                   | 0.225                 | **0.261**              | 0.242                  | 0.235                        | 0.225                  | 0.219                  |\n",
      "| openai_mmlu_por_mcf:philosophy:0                          | 0.238                   | 0.19                  | **0.267**              | 0.193                  | 0.203                        | 0.186                  | 0.196                  |\n",
      "| openai_mmlu_por_mcf:prehistory:0                          | 0.231                   | 0.213                 | 0.235                  | **0.25**               | 0.244                        | 0.216                  | 0.222                  |\n",
      "| openai_mmlu_por_mcf:professional_accounting:0             | 0.238                   | 0.23                  | 0.27                   | **0.284**              | 0.28                         | 0.234                  | 0.234                  |\n",
      "| openai_mmlu_por_mcf:professional_law:0                    | 0.231                   | 0.246                 | **0.254**              | 0.246                  | 0.252                        | 0.246                  | 0.247                  |\n",
      "| openai_mmlu_por_mcf:professional_medicine:0               | 0.228                   | 0.188                 | **0.272**              | 0.25                   | 0.199                        | 0.184                  | 0.18                   |\n",
      "| openai_mmlu_por_mcf:professional_psychology:0             | 0.235                   | 0.24                  | 0.245                  | 0.217                  | **0.255**                    | 0.25                   | 0.248                  |\n",
      "| openai_mmlu_por_mcf:public_relations:0                    | 0.227                   | 0.218                 | 0.209                  | **0.282**              | 0.218                        | 0.218                  | 0.227                  |\n",
      "| openai_mmlu_por_mcf:security_studies:0                    | 0.241                   | 0.192                 | 0.249                  | **0.31**               | 0.196                        | 0.188                  | 0.188                  |\n",
      "| openai_mmlu_por_mcf:sociology:0                           | 0.244                   | 0.264                 | **0.274**              | 0.269                  | 0.244                        | 0.244                  | 0.244                  |\n",
      "| openai_mmlu_por_mcf:us_foreign_policy:0                   | 0.24                    | 0.28                  | 0.26                   | 0.27                   | **0.33**                     | 0.28                   | 0.28                   |\n",
      "| openai_mmlu_por_mcf:virology:0                            | 0.259                   | 0.283                 | 0.223                  | 0.211                  | 0.259                        | 0.283                  | **0.289**              |\n",
      "| openai_mmlu_por_mcf:world_religions:0                     | 0.281                   | 0.31                  | 0.298                  | 0.316                  | 0.287                        | 0.322                  | **0.333**              |\n",
      "| exams_por_mcf:biology:0                                   | 0.239                   | 0.227                 | 0.256                  | **0.295**              | 0.193                        | 0.233                  | 0.233                  |\n",
      "| exams_por_mcf:economics:0                                 | 0.18                    | **0.279**             | 0.252                  | 0.207                  | 0.261                        | **0.279**              | **0.279**              |\n",
      "| exams_por_mcf:geology:0                                   | **0.276**               | 0.241                 | 0.224                  | 0.233                  | 0.233                        | 0.241                  | 0.241                  |\n",
      "| exams_por_mcf:philosophy:0                                | 0.2                     | 0.133                 | **0.267**              | 0.167                  | 0.167                        | 0.133                  | 0.133                  |\n",
      "| oab_exams_por_mcf:0                                       | 0.245                   | 0.23                  | 0.243                  | **0.247**              | 0.231                        | 0.23                   | 0.229                  |\n",
      "| enem_por_mcf:_average:0                                   | **0.216**               | 0.203                 | 0.199                  | 0.197                  | 0.201                        | 0.207                  | 0.209                  |\n",
      "| openai_mmlu_por_mcf:_average:0                            | 0.248                   | 0.231                 | **0.262**              | 0.254                  | 0.239                        | 0.231                  | 0.234                  |\n",
      "| exams_por_mcf:_average:0                                  | 0.224                   | 0.22                  | **0.25**               | 0.226                  | 0.213                        | 0.222                  | 0.222                  |\n",
      "| all                                                       | 0.245                   | 0.229                 | **0.257**              | 0.249                  | 0.235                        | 0.229                  | 0.231                  |\n"
     ]
    }
   ],
   "source": [
    "#df['max'] = df.apply(lambda x: x.max(), axis=1)\n",
    "dft = df.T\n",
    "\n",
    "for c in dft.columns:\n",
    "    # use 6 points floating for better markdown visibility\n",
    "    dft[c] = dft[c].apply(lambda x: f\"**{x:.3}**\" if x == dft[c].max() else f\"{x:.3}\")\n",
    "#df\n",
    "print(dft.T.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48d0daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                            |      all |\n",
      "|:---------------------------|---------:|\n",
      "| google/gemma-3-1B-pt       | 0.256818 |\n",
      "| TucanoBR/Tucano-630m       | 0.248854 |\n",
      "| Fazzioni/biatron-345m      | 0.24493  |\n",
      "| HuggingFaceTB/SmolLM2-360M | 0.23468  |\n",
      "| TucanoBR/Tucano-160m       | 0.231208 |\n",
      "| Qwen/Qwen3-0.6B-Base       | 0.228958 |\n",
      "| google/gemma-3-270m        | 0.228701 |\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(bench).tail(1).T.sort_values('all', ascending=False).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c7416ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 13 09:42:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 Ti     On  |   00000000:05:00.0 Off |                  N/A |\n",
      "|  0%   50C    P2             26W /  165W |    3413MiB /  16380MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1079      G   /usr/lib/xorg/Xorg                        9MiB |\n",
      "|    0   N/A  N/A            1288      G   ...c/gnome-remote-desktop-daemon          2MiB |\n",
      "|    0   N/A  N/A            1326      G   /usr/bin/gnome-shell                      3MiB |\n",
      "|    0   N/A  N/A         1258795      C   ...EIA/MEGATRON/.venv/bin/python       3362MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc03587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
