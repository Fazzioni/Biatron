{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba2c4835",
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install lighteval\n",
    "#!pip install \"lighteval[all]\"\n",
    "#!pip install langdetect\n",
    "#!pip install lighteval[multilingual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b75a49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from biatron import BiatronForCausalLM, BiatronConfig\n",
    "#AutoConfig.register(\"Biatron\", BiatronConfig)\n",
    "#AutoModelForCausalLM.register(BiatronConfig, BiatronForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e642ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "bench  = {}\n",
    "\n",
    "#biamodel = BiatronForCausalLM.from_pretrained(\"\", torch_dtype=torch.bfloat16, device_map=\"cuda\", use_cache=False, _attn_implementation='sdpa', revision=\"checkpoint-152000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
    "from lighteval.models.transformers.transformers_model import TransformersModel, TransformersModelConfig\n",
    "from lighteval.pipeline import ParallelismManager, Pipeline, PipelineParameters\n",
    "\n",
    "BENCHMARKS = \"enem_por_mcf,oab_exams_por_mcf,exams_por_mcf,m3exams_por_mcf,openai_mmlu_por_mcf\"\n",
    "\n",
    "for model_name in ['Fazzioni/biatron-345m',\n",
    "                   'google/gemma-3-270m',\n",
    "                   'google/gemma-3-1B-pt',\n",
    "                   'TucanoBR/Tucano-630m',\n",
    "                   'HuggingFaceTB/SmolLM2-360M',\n",
    "                   'Qwen/Qwen3-0.6B-Base'\n",
    "                   ]:\n",
    "    print(\"STARTING EVAL FOR MODEL:\", model_name)\n",
    "    \n",
    "    evaluation_tracker = EvaluationTracker(output_dir=\"./results\")\n",
    "    pipeline_params = PipelineParameters(\n",
    "        launcher_type=ParallelismManager.NONE,\n",
    "        load_tasks_multilingual=True\n",
    "    )\n",
    "    \n",
    "    CLASS_NAME = AutoModelForCausalLM if 'biatron' not in model_name else BiatronForCausalLM\n",
    "    kwargs = {}\n",
    "    if 'biatron' in model_name:\n",
    "        kwargs = {'revision':'checkpoint-152000'}\n",
    "        \n",
    "    model = CLASS_NAME.from_pretrained(model_name, device_map=\"cuda\", dtype=torch.bfloat16, **kwargs)\n",
    "    \n",
    "    config = TransformersModelConfig(model_name=model.config._name_or_path, batch_size=1)\n",
    "    Transmodel = TransformersModel.from_model(model, config)\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        model=Transmodel,\n",
    "        pipeline_parameters=pipeline_params,\n",
    "        evaluation_tracker=evaluation_tracker,\n",
    "        tasks=BENCHMARKS,\n",
    "    )\n",
    "    \n",
    "    results = pipeline.evaluate()\n",
    "    pipeline.show_results()\n",
    "    results = pipeline.get_results()\n",
    "\n",
    "    #m = model.config._name_or_path\n",
    "    bench[model_name] = {}\n",
    "    for k,v in results['results'].items():\n",
    "        bench[model_name][k] = v['acc']\n",
    "\n",
    "    del model\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4378476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Fazzioni/biatron-345m', 'google/gemma-3-270m', 'google/gemma-3-1B-pt', 'TucanoBR/Tucano-630m', 'HuggingFaceTB/SmolLM2-360M', 'Qwen/Qwen3-0.6B-Base'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17978972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                                           |   Fazzioni/biatron-345m |   google/gemma-3-270m |   google/gemma-3-1B-pt |   TucanoBR/Tucano-630m |   HuggingFaceTB/SmolLM2-360M |   Qwen/Qwen3-0.6B-Base |\n",
      "|:----------------------------------------------------------|------------------------:|----------------------:|-----------------------:|-----------------------:|-----------------------------:|-----------------------:|\n",
      "| m3exams_por_mcf:0                                         |                0.225    |              0.202273 |               0.196591 |               0.201136 |                     0.197727 |               0.197727 |\n",
      "| enem_por_mcf:2022:0                                       |                0.212291 |              0.195531 |               0.206704 |               0.178771 |                     0.195531 |               0.206704 |\n",
      "| enem_por_mcf:2023:0                                       |                0.24581  |              0.24581  |               0.173184 |               0.206704 |                     0.234637 |               0.240223 |\n",
      "| enem_por_mcf:2024:0                                       |                0.189944 |              0.167598 |               0.217877 |               0.206704 |                     0.173184 |               0.173184 |\n",
      "| openai_mmlu_por_mcf:abstract_algebra:0                    |                0.31     |              0.22     |               0.31     |               0.2      |                     0.24     |               0.22     |\n",
      "| openai_mmlu_por_mcf:anatomy:0                             |                0.237037 |              0.185185 |               0.288889 |               0.244444 |                     0.222222 |               0.185185 |\n",
      "| openai_mmlu_por_mcf:astronomy:0                           |                0.223684 |              0.190789 |               0.328947 |               0.230263 |                     0.197368 |               0.177632 |\n",
      "| openai_mmlu_por_mcf:business_ethics:0                     |                0.23     |              0.3      |               0.19     |               0.25     |                     0.22     |               0.3      |\n",
      "| openai_mmlu_por_mcf:clinical_knowledge:0                  |                0.245283 |              0.211321 |               0.241509 |               0.264151 |                     0.203774 |               0.215094 |\n",
      "| openai_mmlu_por_mcf:college_biology:0                     |                0.263889 |              0.243056 |               0.243056 |               0.243056 |                     0.236111 |               0.256944 |\n",
      "| openai_mmlu_por_mcf:college_chemistry:0                   |                0.28     |              0.21     |               0.36     |               0.33     |                     0.22     |               0.2      |\n",
      "| openai_mmlu_por_mcf:college_computer_science:0            |                0.32     |              0.27     |               0.32     |               0.28     |                     0.25     |               0.26     |\n",
      "| openai_mmlu_por_mcf:college_mathematics:0                 |                0.24     |              0.21     |               0.27     |               0.26     |                     0.25     |               0.21     |\n",
      "| openai_mmlu_por_mcf:college_medicine:0                    |                0.231214 |              0.202312 |               0.312139 |               0.242775 |                     0.196532 |               0.208092 |\n",
      "| openai_mmlu_por_mcf:college_physics:0                     |                0.22549  |              0.215686 |               0.362745 |               0.313725 |                     0.205882 |               0.215686 |\n",
      "| openai_mmlu_por_mcf:computer_security:0                   |                0.27     |              0.28     |               0.22     |               0.21     |                     0.28     |               0.28     |\n",
      "| openai_mmlu_por_mcf:conceptual_physics:0                  |                0.26383  |              0.27234  |               0.187234 |               0.178723 |                     0.246809 |               0.26383  |\n",
      "| openai_mmlu_por_mcf:econometrics:0                        |                0.192982 |              0.22807  |               0.219298 |               0.184211 |                     0.219298 |               0.236842 |\n",
      "| openai_mmlu_por_mcf:electrical_engineering:0              |                0.262069 |              0.227586 |               0.275862 |               0.186207 |                     0.262069 |               0.241379 |\n",
      "| openai_mmlu_por_mcf:elementary_mathematics:0              |                0.232804 |              0.198413 |               0.238095 |               0.251323 |                     0.224868 |               0.208995 |\n",
      "| openai_mmlu_por_mcf:formal_logic:0                        |                0.222222 |              0.269841 |               0.301587 |               0.253968 |                     0.293651 |               0.285714 |\n",
      "| openai_mmlu_por_mcf:global_facts:0                        |                0.26     |              0.19     |               0.34     |               0.21     |                     0.24     |               0.18     |\n",
      "| openai_mmlu_por_mcf:high_school_biology:0                 |                0.245161 |              0.187097 |               0.251613 |               0.3      |                     0.187097 |               0.177419 |\n",
      "| openai_mmlu_por_mcf:high_school_chemistry:0               |                0.20197  |              0.152709 |               0.285714 |               0.256158 |                     0.17734  |               0.152709 |\n",
      "| openai_mmlu_por_mcf:high_school_computer_science:0        |                0.35     |              0.25     |               0.3      |               0.22     |                     0.23     |               0.25     |\n",
      "| openai_mmlu_por_mcf:high_school_european_history:0        |                0.212121 |              0.218182 |               0.206061 |               0.230303 |                     0.230303 |               0.218182 |\n",
      "| openai_mmlu_por_mcf:high_school_geography:0               |                0.348485 |              0.181818 |               0.272727 |               0.30303  |                     0.20202  |               0.176768 |\n",
      "| openai_mmlu_por_mcf:high_school_government_and_politics:0 |                0.274611 |              0.196891 |               0.300518 |               0.305699 |                     0.227979 |               0.196891 |\n",
      "| openai_mmlu_por_mcf:high_school_macroeconomics:0          |                0.279487 |              0.210256 |               0.225641 |               0.287179 |                     0.230769 |               0.202564 |\n",
      "| openai_mmlu_por_mcf:high_school_mathematics:0             |                0.248148 |              0.225926 |               0.281481 |               0.281481 |                     0.233333 |               0.211111 |\n",
      "| openai_mmlu_por_mcf:high_school_microeconomics:0          |                0.252101 |              0.205882 |               0.247899 |               0.323529 |                     0.222689 |               0.210084 |\n",
      "| openai_mmlu_por_mcf:high_school_physics:0                 |                0.218543 |              0.205298 |               0.258278 |               0.278146 |                     0.198675 |               0.198675 |\n",
      "| openai_mmlu_por_mcf:high_school_psychology:0              |                0.245872 |              0.19633  |               0.26055  |               0.238532 |                     0.201835 |               0.192661 |\n",
      "| openai_mmlu_por_mcf:high_school_statistics:0              |                0.199074 |              0.180556 |               0.268519 |               0.425926 |                     0.157407 |               0.152778 |\n",
      "| openai_mmlu_por_mcf:high_school_us_history:0              |                0.27451  |              0.240196 |               0.245098 |               0.264706 |                     0.25     |               0.25     |\n",
      "| openai_mmlu_por_mcf:high_school_world_history:0           |                0.223629 |              0.265823 |               0.291139 |               0.244726 |                     0.265823 |               0.270042 |\n",
      "| openai_mmlu_por_mcf:human_aging:0                         |                0.255605 |              0.29148  |               0.26009  |               0.313901 |                     0.286996 |               0.313901 |\n",
      "| openai_mmlu_por_mcf:human_sexuality:0                     |                0.290076 |              0.236641 |               0.206107 |               0.198473 |                     0.259542 |               0.259542 |\n",
      "| openai_mmlu_por_mcf:international_law:0                   |                0.198347 |              0.239669 |               0.140496 |               0.214876 |                     0.206612 |               0.239669 |\n",
      "| openai_mmlu_por_mcf:jurisprudence:0                       |                0.231481 |              0.25     |               0.175926 |               0.259259 |                     0.268519 |               0.259259 |\n",
      "| openai_mmlu_por_mcf:logical_fallacies:0                   |                0.233129 |              0.214724 |               0.294479 |               0.233129 |                     0.214724 |               0.220859 |\n",
      "| openai_mmlu_por_mcf:machine_learning:0                    |                0.258929 |              0.276786 |               0.214286 |               0.205357 |                     0.330357 |               0.3125   |\n",
      "| openai_mmlu_por_mcf:management:0                          |                0.23301  |              0.184466 |               0.252427 |               0.203883 |                     0.203883 |               0.174757 |\n",
      "| openai_mmlu_por_mcf:marketing:0                           |                0.196581 |              0.286325 |               0.247863 |               0.24359  |                     0.269231 |               0.290598 |\n",
      "| openai_mmlu_por_mcf:medical_genetics:0                    |                0.23     |              0.33     |               0.32     |               0.29     |                     0.35     |               0.3      |\n",
      "| openai_mmlu_por_mcf:miscellaneous:0                       |                0.257982 |              0.240102 |               0.268199 |               0.208174 |                     0.261814 |               0.237548 |\n",
      "| openai_mmlu_por_mcf:moral_disputes:0                      |                0.268786 |              0.254335 |               0.265896 |               0.222543 |                     0.280347 |               0.248555 |\n",
      "| openai_mmlu_por_mcf:moral_scenarios:0                     |                0.272626 |              0.24581  |               0.246927 |               0.258101 |                     0.246927 |               0.237989 |\n",
      "| openai_mmlu_por_mcf:nutrition:0                           |                0.248366 |              0.22549  |               0.261438 |               0.24183  |                     0.235294 |               0.22549  |\n",
      "| openai_mmlu_por_mcf:philosophy:0                          |                0.237942 |              0.189711 |               0.266881 |               0.192926 |                     0.202572 |               0.186495 |\n",
      "| openai_mmlu_por_mcf:prehistory:0                          |                0.231481 |              0.212963 |               0.234568 |               0.25     |                     0.243827 |               0.216049 |\n",
      "| openai_mmlu_por_mcf:professional_accounting:0             |                0.237589 |              0.230496 |               0.269504 |               0.283688 |                     0.280142 |               0.234043 |\n",
      "| openai_mmlu_por_mcf:professional_law:0                    |                0.231421 |              0.246415 |               0.254237 |               0.245763 |                     0.25163  |               0.245763 |\n",
      "| openai_mmlu_por_mcf:professional_medicine:0               |                0.227941 |              0.1875   |               0.272059 |               0.25     |                     0.198529 |               0.183824 |\n",
      "| openai_mmlu_por_mcf:professional_psychology:0             |                0.235294 |              0.240196 |               0.245098 |               0.21732  |                     0.254902 |               0.25     |\n",
      "| openai_mmlu_por_mcf:public_relations:0                    |                0.227273 |              0.218182 |               0.209091 |               0.281818 |                     0.218182 |               0.218182 |\n",
      "| openai_mmlu_por_mcf:security_studies:0                    |                0.240816 |              0.191837 |               0.24898  |               0.310204 |                     0.195918 |               0.187755 |\n",
      "| openai_mmlu_por_mcf:sociology:0                           |                0.243781 |              0.263682 |               0.273632 |               0.268657 |                     0.243781 |               0.243781 |\n",
      "| openai_mmlu_por_mcf:us_foreign_policy:0                   |                0.24     |              0.28     |               0.26     |               0.27     |                     0.33     |               0.28     |\n",
      "| openai_mmlu_por_mcf:virology:0                            |                0.259036 |              0.283133 |               0.222892 |               0.210843 |                     0.259036 |               0.283133 |\n",
      "| openai_mmlu_por_mcf:world_religions:0                     |                0.280702 |              0.309942 |               0.298246 |               0.315789 |                     0.28655  |               0.321637 |\n",
      "| exams_por_mcf:biology:0                                   |                0.238636 |              0.227273 |               0.255682 |               0.295455 |                     0.193182 |               0.232955 |\n",
      "| exams_por_mcf:economics:0                                 |                0.18018  |              0.279279 |               0.252252 |               0.207207 |                     0.261261 |               0.279279 |\n",
      "| exams_por_mcf:geology:0                                   |                0.275862 |              0.241379 |               0.224138 |               0.232759 |                     0.232759 |               0.241379 |\n",
      "| exams_por_mcf:philosophy:0                                |                0.2      |              0.133333 |               0.266667 |               0.166667 |                     0.166667 |               0.133333 |\n",
      "| oab_exams_por_mcf:0                                       |                0.245249 |              0.230317 |               0.242986 |               0.246606 |                     0.230769 |               0.229864 |\n",
      "| enem_por_mcf:_average:0                                   |                0.216015 |              0.20298  |               0.199255 |               0.197393 |                     0.201117 |               0.206704 |\n",
      "| openai_mmlu_por_mcf:_average:0                            |                0.248288 |              0.231078 |               0.261648 |               0.254077 |                     0.238652 |               0.231169 |\n",
      "| exams_por_mcf:_average:0                                  |                0.22367  |              0.220316 |               0.249685 |               0.225522 |                     0.213467 |               0.221737 |\n",
      "| all                                                       |                0.24493  |              0.228701 |               0.256818 |               0.248854 |                     0.23468  |               0.228958 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(bench).to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
